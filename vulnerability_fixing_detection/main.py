#!/usr/bin/env python3
"""
Main entry point for the Vulnerability Fixing Detection Pipeline.

This script provides a command-line interface for running the vulnerability
fixing detection pipeline with various configuration options.
"""

import argparse
import sys
import os
import csv
import time
import signal
import multiprocessing
from datetime import datetime
from tqdm import tqdm

from config.settings import load_config, DetectionConfig
from pipeline import VulnerabilityDetectionPipeline
from utils.csv_file_utils import CSVFileManager
from utils.logging_utils import setup_logger

# Set up main logger
logger = setup_logger(__name__)


def create_parser() -> argparse.ArgumentParser:
    """Create command line argument parser."""
    parser = argparse.ArgumentParser(
        description="Detect vulnerability fixes in code changes using multi-agent analysis",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage with OpenAI
  python main.py --input input_data.csv --provider openai --model gpt-4o

  # Use Anthropic Claude
  python main.py --input input_data.csv --provider anthropic --model claude-3-sonnet-20240229

  # Use diff format and exclude commit messages
  python main.py --input input_data.csv --use-diff-format --no-commit-message

  # Generate summary report
  python main.py --input input_data.csv --summary-only

  # Filter high-confidence results
  python main.py --input input_data.csv --filter-output --min-score 2
        """
    )

    # Input/Output configuration
    parser.add_argument(
        "--input",
        dest="input_file",
        default="_example_vul_fixing_pairs.csv",
        help="Path to input CSV file (default: input_data.csv)"
    )

    parser.add_argument(
        "--output-dir",
        default="_output",
        help="Directory for output files (default: _output)"
    )

    # Model configuration
    parser.add_argument(
        "--provider",
        choices=["openai", "anthropic"],
        default="openai",
        help="Model provider to use (default: openai)"
    )

    parser.add_argument(
        "--model",
        default="gpt-4o",
        help="Model name to use (default: gpt-4o)"
    )

    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="Temperature for text generation (default: 0.0 for deterministic)"
    )

    # Analysis configuration
    parser.add_argument(
        "--use-diff-format",
        action="store_true",
        help="Use diff format for code changes"
    )

    parser.add_argument(
        "--no-commit-message",
        action="store_true",
        help="Exclude commit message from analysis"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Number of samples to process in each batch (default: 100)"
    )

    # Execution options
    parser.add_argument(
        "--restart-interval",
        type=int,
        default=20,
        help="Restart interval in minutes (default: 20)"
    )

    parser.add_argument(
        "--disable-restart",
        action="store_true",
        help="Disable automatic restart functionality"
    )

    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from previous run (skip already processed samples)"
    )

    # Output options
    parser.add_argument(
        "--summary-only",
        action="store_true",
        help="Generate summary report only (don't process samples)"
    )

    parser.add_argument(
        "--filter-output",
        action="store_true",
        help="Create filtered output with high-confidence results"
    )

    parser.add_argument(
        "--min-score",
        type=int,
        default=2,
        choices=[0, 1, 2, 3],
        help="Minimum consensus score for filtered output (default: 2)"
    )

    # Logging options
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )

    parser.add_argument(
        "--log-file",
        help="Log file path (default: vulnerability_detection.log)"
    )

    return parser


def log_message(message: str) -> None:
    """Log message with timestamp."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")


def validate_environment() -> bool:
    """Validate that required environment variables are set."""
    required_vars = []

    # Check if any API key is available
    if not os.getenv("OPENAI_API_KEY") and not os.getenv("ANTHROPIC_API_KEY"):
        required_vars.append("OPENAI_API_KEY or ANTHROPIC_API_KEY")

    if required_vars:
        logger.error("Missing required environment variables:")
        for var in required_vars:
            logger.error(f"  - {var}")
        return False

    return True


def generate_summary_report(csv_manager: CSVFileManager, output_file: str) -> None:
    """Generate and display summary report."""
    summary = csv_manager.generate_summary_report(output_file)

    if "error" in summary:
        logger.error(f"Error generating summary: {summary['error']}")
        return

    print("\n" + "=" * 60)
    print("VULNERABILITY FIXING DETECTION SUMMARY REPORT")
    print("=" * 60)
    print(f"Total samples processed: {summary['total_samples']}")
    print(f"Vulnerability fix candidates: {summary['vuln_fix_candidates']}")
    print(f"Non-vulnerability fixes: {summary['non_vuln_fixes']}")
    print(f"Uncertain cases: {summary['uncertain_cases']}")

    print("\nConsensus Score Distribution:")
    for score, desc in DetectionConfig.CONSENSUS_SCORES.items():
        count = summary['consensus_distribution'].get(score, 0)
        percentage = summary['consensus_percentages'].get(score, "0.0%")
        print(f"  Score {score}: {count} samples ({percentage}) - {desc}")

    print("\nTop CWE Types:")
    for cwe_id, count in list(summary['cwe_distribution'].items())[:10]:
        cwe_name = DetectionConfig.COMMON_CWE_TYPES.get(cwe_id, "Unknown")
        print(f"  CWE-{cwe_id}: {count} samples - {cwe_name}")

    print("=" * 60)


def process_samples_batch(
        pipeline: VulnerabilityDetectionPipeline,
        csv_manager: CSVFileManager,
        input_file: str,
        output_file: str,
        args: argparse.Namespace
) -> int:
    """Process samples in batches."""

    # Get already processed samples if resuming
    processed_ids = set()
    if args.resume:
        processed_ids = csv_manager.get_processed_identifiers(output_file)

    # Initialize output file
    csv_manager.initialize_output_csv(output_file)

    # Count total samples
    total_samples = csv_manager.count_csv_rows(input_file)
    log_message(f"Total samples in input file: {total_samples}")

    processed_count = 0
    skipped_count = 0

    # Process samples with progress bar
    with tqdm(total=total_samples, desc="Processing samples") as pbar:
        batch_results = []

        for row in csv_manager.read_input_csv(input_file):
            try:
                # Create identifier to check if already processed
                identifier = csv_manager._create_identifier(
                    row.get('func_before', ''),
                    row.get('func_after', ''),
                    row.get('commit_message', ''),
                    row.get('cwe_id', '')
                )

                # Skip if already processed and resuming
                if args.resume and identifier in processed_ids:
                    skipped_count += 1
                    pbar.update(1)
                    continue

                # Skip if no code changes
                if not row.get('func_before', '').strip() or not row.get('func_after', '').strip():
                    skipped_count += 1
                    pbar.update(1)
                    continue

                # Process the sample
                result = pipeline.process_csv_row(
                    row=row,
                    use_diff_format=args.use_diff_format,
                    include_commit_message=not args.no_commit_message
                )

                if result:
                    batch_results.append(result)
                    processed_count += 1

                    # Write batch when it reaches batch_size
                    if len(batch_results) >= args.batch_size:
                        csv_manager.save_batch_results(output_file, batch_results)
                        batch_results = []

                        # Save progress
                        progress_data = {
                            "processed_count": processed_count,
                            "skipped_count": skipped_count,
                            "timestamp": time.time(),
                            "current_sample": processed_count + skipped_count
                        }
                        csv_manager.save_progress(progress_data)

                pbar.update(1)

            except Exception as e:
                logger.error(f"Error processing sample: {e}")
                pbar.update(1)
                continue

        # Write remaining results
        if batch_results:
            csv_manager.save_batch_results(output_file, batch_results)

    # Clean up progress file
    csv_manager.cleanup_progress()

    log_message(f"Processing completed. Processed: {processed_count}, Skipped: {skipped_count}")
    return processed_count


def main_processing_task(args: argparse.Namespace) -> None:
    """Main processing function that can be run in a separate process."""
    try:
        log_message("Starting vulnerability fixing detection pipeline...")

        # Load configuration
        model_config, pipeline_config, api_credentials = load_config(
            provider=args.provider,
            model=args.model,
            temperature=args.temperature,
            output_dir=args.output_dir,
            input_file=args.input_file,
            use_diff_format=args.use_diff_format,
            include_commit_message=not args.no_commit_message,
            restart_interval=args.restart_interval,
            disable_restart=args.disable_restart,
            batch_size=args.batch_size
        )

        # Initialize pipeline
        pipeline = VulnerabilityDetectionPipeline(
            model_config=model_config,
            pipeline_config=pipeline_config,
            api_credentials=api_credentials
        )

        # Initialize CSV manager
        csv_manager = CSVFileManager(args.output_dir)

        # Validate API connection
        if not pipeline.validate_connection():
            logger.error("Failed to validate API connection")
            return

        # Generate output file name
        output_file = os.path.join(args.output_dir, f"detection_output_{args.model}.csv")

        # Handle summary-only mode
        if args.summary_only:
            generate_summary_report(csv_manager, output_file)
            return

        # Process samples
        processed_count = process_samples_batch(
            pipeline=pipeline,
            csv_manager=csv_manager,
            input_file=args.input_file,
            output_file=output_file,
            args=args
        )

        # Generate summary report
        if processed_count > 0:
            generate_summary_report(csv_manager, output_file)

        # Create filtered output if requested
        if args.filter_output:
            filtered_file = output_file.replace('.csv', f'_filtered_score_{args.min_score}.csv')
            csv_manager.create_filtered_output(output_file, filtered_file, args.min_score)
            log_message(f"Filtered output created: {filtered_file}")

        log_message("Pipeline processing completed successfully")

    except Exception as e:
        log_message(f"Fatal error in main processing task: {e}")
        raise


def run_with_restart(args: argparse.Namespace) -> None:
    """Run the main processing with automatic restart functionality."""
    restart_count = 0

    while True:
        restart_count += 1
        log_message(f"Starting processing session #{restart_count}")

        # Create a new process for the main task
        process = multiprocessing.Process(target=main_processing_task, args=(args,))
        process.start()

        # Wait for the restart interval or process completion
        process.join(timeout=args.restart_interval * 60)  # Convert minutes to seconds

        if process.is_alive():
            log_message(f"Restart interval ({args.restart_interval} minutes) reached. Terminating current process...")
            process.terminate()
            process.join(timeout=10)  # Wait up to 10 seconds for graceful termination

            if process.is_alive():
                log_message("Force killing the process...")
                process.kill()
                process.join()

            log_message("Process terminated. Restarting in 5 seconds...")
            time.sleep(5)
        else:
            # Process completed naturally
            if process.exitcode == 0:
                log_message("Processing completed successfully!")
                break
            else:
                log_message(f"Process exited with error code {process.exitcode}. Restarting in 10 seconds...")
                time.sleep(10)


def main():
    """Main entry point."""
    parser = create_parser()
    args = parser.parse_args()

    # Validate environment
    if not validate_environment():
        return 1

    # Configure logging
    import logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logger = setup_logger(
        __name__,
        level=log_level,
        log_file=args.log_file or "vulnerability_detection.log",
        console_output=True
    )

    log_message("=== Vulnerability Fixing Detection Pipeline ===")
    log_message(f"Input file: {args.input_file}")
    log_message(f"Model: {args.model} ({args.provider})")
    log_message(f"Output directory: {args.output_dir}")

    try:
        if args.disable_restart:
            log_message("Auto-restart is DISABLED")
            main_processing_task(args)
        else:
            log_message(f"Auto-restart enabled: every {args.restart_interval} minutes")
            run_with_restart(args)

        return 0

    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"Pipeline failed with error: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    # Handle multiprocessing on Windows
    multiprocessing.freeze_support()
    sys.exit(main())
